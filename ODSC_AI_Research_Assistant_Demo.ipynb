{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4MZwaRkbVcNw",
        "SmeBWd33Vekn",
        "uZkflG4RVo-8",
        "6oyNDKE2V3ZT",
        "ufDX-MpdPr93",
        "rZgHyK06i51-",
        "WgZcTuQapDSP",
        "mwMNfyscpHjT",
        "vbsouuK6q3LA",
        "uIP5opPY0pTM",
        "GOaAIXOE4_RH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atul-ai/ODSC-Agents-Demo/blob/main/ODSC_AI_Research_Assistant_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem**\n",
        "For every scientist, academic, grad student, and engineer - literature review is a daunting task. We only wish we could automate getting insights and simplify work of finding papers and summarizing them. With advent of AI that is a distinct possibility. In this demo we attempt to build a Research Assistant  agent for this.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b9141TGmwseO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DISCLAIMER:**\n",
        "THIS IS NOT A NOVEL IDEA / PROBLEM, WE ARE USING THIS AS AN EXAMPLE TO EXPLAIN CONCEPTS OF BUILDING AGENTS. AGENT BUILT IS NOT MEANT FOR PRODUCTION USE. USE AT YOUR OWN RISK.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-12_LF3yL9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building The Research Agent**\n",
        "\n",
        "#### **LAYOUT:**\n",
        "In this notebook we will be working towards building an agent in a step by step manner. Anthropic released a blog post about build AI Agents and discussed some very important concepts - [Building Effective Agents by Anthropic]( https://www.anthropic.com/engineering/building-effective-agents). We will walk through some critical patterns that help us to build the agent.\n",
        "\n",
        "\n",
        "**Optional:** At the end of the notebook, there is an optional section that shows a succinct version of the agent using the LangGraph constructs to simplify the more education implementation. This would be useful for folks who want to see a succinct implementation.\n",
        "\n",
        "Following are the patterns we will explore:\n",
        "1. The Agumented Model\n",
        "2. Prompt Chaining\n",
        "3. Routing\n",
        "4. Parallelization - **Not Implemented**\n",
        "5. Orchestrator-workers - **Not Implemented**\n",
        "6. Evaluator-optimizer - **Not Implemented**\n",
        "7. The Agent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g_b39R9Z6JYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "4MZwaRkbVcNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-groq langgraph arxiv PyPDF2 requests langchain-openai python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yBJ5ziJmVYp0",
        "outputId": "d3aa770c-4c78-4c11-c89b-5463e2f3d9a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.56)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.38)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.66-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.58-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.76.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langgraph-0.4.1-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.58-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.24.0-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.25-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.66-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=c368a40926edd1a6ba1155bf1bd020beaaa6f805ca3a9e36cbe8450ceffcd512\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, xxhash, python-dotenv, PyPDF2, ormsgpack, feedparser, tiktoken, arxiv, langgraph-sdk, groq, langchain-core, langgraph-checkpoint, langchain-openai, langchain-groq, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "Successfully installed PyPDF2-3.0.1 arxiv-2.2.0 feedparser-6.0.11 groq-0.24.0 langchain-core-0.3.58 langchain-groq-0.3.2 langchain-openai-0.3.16 langgraph-0.4.1 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.66 ormsgpack-1.9.1 python-dotenv-1.1.0 sgmllib3k-1.0.0 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "SmeBWd33Vekn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "import arxiv\n",
        "from langchain.agents.agent import AgentExecutor, AgentAction, AgentFinish\n",
        "from langchain.agents import create_openai_tools_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.tools import Tool, StructuredTool\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "#from langgraph.prebuilt import ToolExecutor\n",
        "import requests\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "import re"
      ],
      "metadata": {
        "id": "rPPjuB-tVg7M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get the Groq API key\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Check if the key was found\n",
        "if not groq_api_key:\n",
        "    raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"Groq API key loaded successfully!\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"OpenAI API key loaded successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYkvzwi2Z4Ct",
        "outputId": "002da780-230b-4e42-8dfc-ec6057239217"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq API key loaded successfully!\n",
            "OpenAI API key loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools Infra\n",
        "\n",
        "Let us create a set of tools which we will give to our LLM to use during its ReAct loop for the agent.\n",
        "\n",
        "\n",
        "We will keep using these tools again and again, to build the patterns called out above. This illustrates the reusability of the tools in AI Application building processes.\n",
        "\n",
        "In a enterprise / production environment - tools will be your existing internal APIs that are datasources that you would like to include as tools in our AI app.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uZkflG4RVo-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Simple ArXiv Search Tool\n",
        "def search_arxiv(query, max_results=5):\n",
        "    \"\"\"Search for papers on arXiv and return basic info.\"\"\"\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(query=query, max_results=max_results)\n",
        "\n",
        "    results = []\n",
        "    for paper in client.results(search):\n",
        "        results.append({\n",
        "            \"id\": paper.entry_id.split(\"/\")[-1],\n",
        "            \"title\": paper.title,\n",
        "            \"authors\": [author.name for author in paper.authors],\n",
        "            \"url\": paper.pdf_url\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "oY1RXAdwVrOT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Simple Paper Downloader\n",
        "def download_paper(paper_id):\n",
        "    \"\"\"Download a paper from arXiv by ID and extract text.\"\"\"\n",
        "    search = arxiv.Search(id_list=[paper_id])\n",
        "    client = arxiv.Client()\n",
        "    paper = next(client.results(search))\n",
        "\n",
        "    response = requests.get(paper.pdf_url)\n",
        "    pdf_file = BytesIO(response.content)\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "\n",
        "    return text[:5000]  # Return first 5000 chars for simplicity"
      ],
      "metadata": {
        "id": "G4e4Ip1uVt1S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Simple Summarizer\n",
        "def summarize_text(text, groq_api_key):\n",
        "    \"\"\"Summarize text using Groq.\"\"\"\n",
        "    model = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "    prompt = f\"Summarize this academic paper: {text[:15000]}\"\n",
        "    return model.invoke(prompt).content"
      ],
      "metadata": {
        "id": "t7oOCwDpVwZl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tools(groq_api_key: str):\n",
        "    \"\"\"Create the tools for our agent.\"\"\"\n",
        "\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"search_arxiv\",\n",
        "            description=\"Search for academic papers on arXiv. Input is a search query.\",\n",
        "            func=lambda q: search_arxiv(q)\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"download_paper\",\n",
        "            description=\"Download a paper from arXiv and extract its text. Input is a paper ID.\",\n",
        "            func=lambda id: download_paper(id)\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"summarize_text\",\n",
        "            description=\"Summarize a piece of text. Input is the text to summarize.\",\n",
        "            func=lambda t: summarize_text(t, groq_api_key)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return tools"
      ],
      "metadata": {
        "id": "4_T9YDXHV2FA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Augmented Model\n",
        "\n",
        "![Anthropic Augmented LLM](https://drive.google.com/uc?export=view&id=1FDId1KGkJB3whzXVLvsW0xPghxVmQ9w2)\n",
        "This is the fundamental building block for building an Agents. The LLM is supplimented with tools and memory. Since we are using LnagChain as our framework, memory construct is inbuilt for us. We are treating search as equivalent to retreival in this case.\n",
        "\n",
        "Let us build a simple version of this pattern.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6oyNDKE2V3ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant that can use tools to answer user questions about research papers.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "def setup_augmented_llm(groq_api_key):\n",
        "    # Define the tools\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # Create the LLM\n",
        "    llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "\n",
        "    # Create the agent with tools\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # Create an agent executor\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    return agent_executor\n",
        "\n",
        "# 5. Run the system\n",
        "def run_augmented_llm(query, groq_api_key):\n",
        "    agent_executor = setup_augmented_llm(groq_api_key)\n",
        "    result = agent_executor.invoke({\"input\": query})\n",
        "    return result[\"output\"]"
      ],
      "metadata": {
        "id": "jtTO0dKNXbAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Augmented LLM - Demo"
      ],
      "metadata": {
        "id": "ufDX-MpdPr93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the latest developments in quantum computing?\"\n",
        "response = run_augmented_llm(query, groq_api_key)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pY8KEpaPwqR",
        "outputId": "05f444df-0a78-43e5-f39f-601a3494efa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_arxiv` with `latest developments in quantum computing 2023`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'id': '2302.00001v1', 'title': 'A Living Review of Quantum Computing for Plasma Physics', 'authors': ['Óscar Amaro', 'Diogo Cruz'], 'url': 'http://arxiv.org/pdf/2302.00001v1'}, {'id': '2304.02088v1', 'title': 'Quantum networks with neutral atom processing nodes', 'authors': ['Jacob P. Covey', 'Harald Weinfurter', 'Hannes Bernien'], 'url': 'http://arxiv.org/pdf/2304.02088v1'}, {'id': '2305.17713v2', 'title': 'Variational Quantum Algorithms for Gibbs State Preparation', 'authors': ['Mirko Consiglio'], 'url': 'http://arxiv.org/pdf/2305.17713v2'}, {'id': '2306.03608v1', 'title': 'A Survey of Quantum-Cognitively Inspired Sentiment Analysis Models', 'authors': ['Yaochen Liu', 'Qiuchi Li', 'Benyou Wang', 'Yazhou Zhang', 'Dawei Song'], 'url': 'http://arxiv.org/pdf/2306.03608v1'}, {'id': '2304.02201v1', 'title': 'Estimating Patterns of Classical and Quantum Skyrmion States', 'authors': ['Vladimir V. Mazurenko', 'Ilia A. Iakovlev', 'Oleg M. Sotnikov', 'Mikhail I. Katsnelson'], 'url': 'http://arxiv.org/pdf/2304.02201v1'}]\u001b[0m\u001b[32;1m\u001b[1;3mSeveral exciting advances have emerged recently in quantum computing, both on the hardware and algorithmic fronts. Here are some of the key developments and trends:\n",
            "\n",
            "1. Quantum Networking and Hardware Innovations:  \n",
            " • Researchers have been exploring new architectures for quantum networks. For example, recent work on quantum networks with neutral atom processing nodes proposes designs in which neutral atoms serve as robust qubits. This approach can potentially yield scalable, high-fidelity quantum connections that are essential for building distributed quantum systems.  \n",
            " • Advances in qubit control (whether with superconducting circuits, trapped ions, or neutral atoms) are helping push the boundaries of coherence times and gate fidelities.\n",
            "\n",
            "2. Improved Quantum Algorithms and State Preparation Techniques:  \n",
            " • There is significant interest in developing variational quantum algorithms tailored for tasks such as Gibbs state preparation. These techniques are geared toward harnessing near-term quantum devices (often called Noisy Intermediate-Scale Quantum, or NISQ, devices) to simulate complex systems and sample from thermal distributions.\n",
            " • The refinement of quantum algorithms aims not only to take advantage of quantum hardware improvements but also to mitigate noise and errors that occur during computation.\n",
            "\n",
            "3. Applications-Oriented Reviews and Cross-Disciplinary Efforts:  \n",
            " • In some reviews, such as the discussion on quantum computing for plasma physics, the focus is on applying quantum computing methods to challenging problems in other fields like high-energy density physics. This cross-disciplinary approach is expanding the impact of quantum computing beyond traditional areas.\n",
            "\n",
            "4. Error Mitigation and Quantum Control Advances:  \n",
            " • Along with developing more robust quantum hardware, the field is also seeing progress in error correction and mitigation techniques. These developments are crucial as they allow existing quantum processors to run more complex algorithms with better reliability.\n",
            "\n",
            "Overall, the current trajectory in quantum computing involves a close interplay between refining physical implementations (like those using neutral atoms) and advancing theoretical methods (such as variational algorithms for state preparation). These joint efforts are steadily building toward more scalable, fault-tolerant quantum machines capable of solving practical and computationally hard problems. \n",
            "\n",
            "If you’re interested in exploring these works in more detail, one might look at recent arXiv submissions such as “Quantum networks with neutral atom processing nodes” and “Variational Quantum Algorithms for Gibbs State Preparation,” among others. Each of these works touches on a different facet of the field—from hardware proposals to algorithm design—that collectively define the state of the art in quantum computing.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Several exciting advances have emerged recently in quantum computing, both on the hardware and algorithmic fronts. Here are some of the key developments and trends:\n",
            "\n",
            "1. Quantum Networking and Hardware Innovations:  \n",
            " • Researchers have been exploring new architectures for quantum networks. For example, recent work on quantum networks with neutral atom processing nodes proposes designs in which neutral atoms serve as robust qubits. This approach can potentially yield scalable, high-fidelity quantum connections that are essential for building distributed quantum systems.  \n",
            " • Advances in qubit control (whether with superconducting circuits, trapped ions, or neutral atoms) are helping push the boundaries of coherence times and gate fidelities.\n",
            "\n",
            "2. Improved Quantum Algorithms and State Preparation Techniques:  \n",
            " • There is significant interest in developing variational quantum algorithms tailored for tasks such as Gibbs state preparation. These techniques are geared toward harnessing near-term quantum devices (often called Noisy Intermediate-Scale Quantum, or NISQ, devices) to simulate complex systems and sample from thermal distributions.\n",
            " • The refinement of quantum algorithms aims not only to take advantage of quantum hardware improvements but also to mitigate noise and errors that occur during computation.\n",
            "\n",
            "3. Applications-Oriented Reviews and Cross-Disciplinary Efforts:  \n",
            " • In some reviews, such as the discussion on quantum computing for plasma physics, the focus is on applying quantum computing methods to challenging problems in other fields like high-energy density physics. This cross-disciplinary approach is expanding the impact of quantum computing beyond traditional areas.\n",
            "\n",
            "4. Error Mitigation and Quantum Control Advances:  \n",
            " • Along with developing more robust quantum hardware, the field is also seeing progress in error correction and mitigation techniques. These developments are crucial as they allow existing quantum processors to run more complex algorithms with better reliability.\n",
            "\n",
            "Overall, the current trajectory in quantum computing involves a close interplay between refining physical implementations (like those using neutral atoms) and advancing theoretical methods (such as variational algorithms for state preparation). These joint efforts are steadily building toward more scalable, fault-tolerant quantum machines capable of solving practical and computationally hard problems. \n",
            "\n",
            "If you’re interested in exploring these works in more detail, one might look at recent arXiv submissions such as “Quantum networks with neutral atom processing nodes” and “Variational Quantum Algorithms for Gibbs State Preparation,” among others. Each of these works touches on a different facet of the field—from hardware proposals to algorithm design—that collectively define the state of the art in quantum computing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Chaining\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1Rd_jjg1OaCxnvHFGszcwmaSALh1ZWVII)\n",
        "\n",
        "This workflow pattern chains output of one LLM Call / Tool to the other one.\n",
        "\n",
        "The research assistant is the orchestrator that coordinates the search and synthesis. search_llm and synthesis_llm are the two LLMs in this workflow. The output of search_llm is fed into the input of synthesis_llm. search_llm is used to convert the user's question into a search query using openai. synthesis_llm is used to synthesize the results of the search using groq.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rZgHyK06i51-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Prompts"
      ],
      "metadata": {
        "id": "WgZcTuQapDSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_prompt_chain(groq_api_key, openai_api_key):\n",
        "    \"\"\"\n",
        "    Setup a chain of prompts where the output of one LLM call is fed into another.\n",
        "    This demonstrates the Prompt Chaining pattern.\n",
        "    \"\"\"\n",
        "    # Create the LLMs\n",
        "    search_llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "    synthesis_llm = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "\n",
        "    # Create the tools\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # STEP 1: Create the search agent prompt\n",
        "    search_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research assistant that helps find relevant papers on arXiv.\n",
        "Your job is to convert a user's research question into the best possible search query for arXiv.\n",
        "Focus on extracting key technical terms and concepts.\n",
        "Respond ONLY with the search query, nothing else.\"\"\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 2: Create the paper selection agent prompt\n",
        "    selection_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research assistant that helps select the most relevant paper from search results.\n",
        "Given a list of papers and the original research question, select the SINGLE most relevant paper ID.\n",
        "Respond ONLY with the paper ID, nothing else.\"\"\"),\n",
        "        (\"human\", \"Research question: {original_query}\\nSearch results: {search_results}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 3: Create the analysis agent prompt\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research expert that analyzes academic papers.\n",
        "Given the text of a paper, identify the key methodologies, findings, and implications.\n",
        "Focus on how this paper addresses the user's original question.\"\"\"),\n",
        "        (\"human\", \"Original question: {original_query}\\nPaper text: {paper_text}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 4: Create the final synthesis prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful AI research assistant that synthesizes information into clear,\n",
        "concise responses. Create a comprehensive yet accessible answer to the user's question based on the\n",
        "technical analysis provided.\"\"\"),\n",
        "        (\"human\", \"Original question: {original_query}\\nTechnical analysis: {analysis}\")\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"search_llm\": search_llm,\n",
        "        \"synthesis_llm\": synthesis_llm,\n",
        "        \"tools\": tools,\n",
        "        \"prompts\": {\n",
        "            \"search\": search_prompt,\n",
        "            \"selection\": selection_prompt,\n",
        "            \"analysis\": analysis_prompt,\n",
        "            \"synthesis\": synthesis_prompt\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "JTxnmYL1nkGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Chaining"
      ],
      "metadata": {
        "id": "mwMNfyscpHjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_prompt_chain(query, groq_api_key, openai_api_key):\n",
        "    \"\"\"\n",
        "    Execute the entire prompt chain to answer a research question.\n",
        "    This demonstrates how outputs from one LLM call are fed as inputs to the next.\n",
        "    \"\"\"\n",
        "    # Setup the chain components\n",
        "    chain = setup_prompt_chain(groq_api_key, openai_api_key)\n",
        "    search_llm = chain[\"search_llm\"]\n",
        "    synthesis_llm = chain[\"synthesis_llm\"]\n",
        "    tools = chain[\"tools\"]\n",
        "    prompts = chain[\"prompts\"]\n",
        "\n",
        "    print(\"📝 STARTING PROMPT CHAIN\")\n",
        "    print(f\"📋 Original query: {query}\")\n",
        "\n",
        "    # STEP 1: Convert question to search query\n",
        "    print(\"\\n🔍 Step 1: Converting question to optimal search query...\")\n",
        "    search_query = search_llm.invoke(prompts[\"search\"].format(input=query)).content\n",
        "    print(f\"🔍 Generated search query: {search_query}\")\n",
        "\n",
        "    # STEP 2: Search for papers\n",
        "    print(\"\\n📚 Step 2: Searching for papers on arXiv...\")\n",
        "    search_results = search_arxiv(search_query)\n",
        "    print(f\"📚 Found {len(search_results)} papers\")\n",
        "\n",
        "    # STEP 3: Select the most relevant paper\n",
        "    print(\"\\n🔎 Step 3: Selecting the most relevant paper...\")\n",
        "    paper_id = search_llm.invoke(\n",
        "        prompts[\"selection\"].format(\n",
        "            original_query=query,\n",
        "            search_results=search_results\n",
        "        )\n",
        "    ).content\n",
        "    print(f\"🔎 Selected paper ID: {paper_id}\")\n",
        "\n",
        "    # STEP 4: Download the selected paper\n",
        "    print(\"\\n📄 Step 4: Downloading the selected paper...\")\n",
        "    paper_text = download_paper(paper_id)\n",
        "    print(f\"📄 Downloaded {len(paper_text)} characters of paper text\")\n",
        "\n",
        "    # STEP 5: Analyze the paper\n",
        "    print(\"\\n🧪 Step 5: Analyzing the paper...\")\n",
        "    analysis = search_llm.invoke(\n",
        "        prompts[\"analysis\"].format(\n",
        "            original_query=query,\n",
        "            paper_text=paper_text\n",
        "        )\n",
        "    ).content\n",
        "    print(f\"🧪 Analysis complete\")\n",
        "\n",
        "    # STEP 6: Final synthesis\n",
        "    print(\"\\n🔮 Step 6: Synthesizing final response...\")\n",
        "    final_response = synthesis_llm.invoke(\n",
        "        prompts[\"synthesis\"].format(\n",
        "            original_query=query,\n",
        "            analysis=analysis\n",
        "        )\n",
        "    ).content\n",
        "\n",
        "    print(\"\\n✅ PROMPT CHAIN COMPLETE\")\n",
        "\n",
        "    return {\n",
        "        \"search_query\": search_query,\n",
        "        \"selected_paper\": next((p for p in search_results if p[\"id\"] == paper_id), None),\n",
        "        \"analysis\": analysis,\n",
        "        \"response\": final_response\n",
        "    }"
      ],
      "metadata": {
        "id": "rZa050dLnnkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Run the prompt chain\n",
        "    query = \"What are the latest breakthroughs in quantum error correction?\"\n",
        "    result = run_prompt_chain(query, groq_api_key, openai_api_key)\n",
        "\n",
        "    # Output the final response\n",
        "    print(\"\\n----- FINAL RESPONSE -----\")\n",
        "    print(result[\"response\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxsG3eqfnqPT",
        "outputId": "d8b6adf4-73a8-4a96-bd6b-7310ecf023d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 STARTING PROMPT CHAIN\n",
            "📋 Original query: What are the latest breakthroughs in quantum error correction?\n",
            "\n",
            "🔍 Step 1: Converting question to optimal search query...\n",
            "🔍 Generated search query: quantum error correction AND breakthroughs AND advancements AND recent\n",
            "\n",
            "📚 Step 2: Searching for papers on arXiv...\n",
            "📚 Found 5 papers\n",
            "\n",
            "🔎 Step 3: Selecting the most relevant paper...\n",
            "🔎 Selected paper ID: 2309.16104v2\n",
            "\n",
            "📄 Step 4: Downloading the selected paper...\n",
            "📄 Downloaded 5000 characters of paper text\n",
            "\n",
            "🧪 Step 5: Analyzing the paper...\n",
            "🧪 Analysis complete\n",
            "\n",
            "🔮 Step 6: Synthesizing final response...\n",
            "\n",
            "✅ PROMPT CHAIN COMPLETE\n",
            "\n",
            "----- FINAL RESPONSE -----\n",
            "The latest breakthroughs in quantum error correction involve the development of a new construction for geometrically local quantum codes that achieves optimal code parameters and an optimal energy barrier. This is a significant advancement because it simplifies the construction process, making it more feasible for practical implementation.\n",
            "\n",
            "The breakthrough is based on the use of \"balanced product codes,\" a type of quantum low-density parity-check (qLDPC) code with a natural 2D structure. By leveraging this structure, the researchers bypassed a previously necessary complex mathematical step, making the construction more explicit and easier to analyze.\n",
            "\n",
            "The new code construction achieves two key milestones:\n",
            "\n",
            "1. **Optimal code parameters**: It strikes the optimal balance between code dimension and distance, meeting the upper bound up to polylogarithmic factors.\n",
            "2. **Optimal energy barrier**: It attains an optimal energy barrier, a crucial property for practical quantum error correction, as it ensures stability against noise.\n",
            "\n",
            "These results have far-reaching implications for both quantum and classical error-correcting codes. The breakthrough is particularly significant for quantum computing, as it:\n",
            "\n",
            "* **Enables scalable quantum computing architectures**: By achieving geometrical locality, the construction facilitates short-range interactions in physical systems, a necessary condition for scalable quantum computing.\n",
            "* **Enhances resilience to noise**: The optimal energy barrier suggests that these codes could be more reliable in the face of noise, bolstering the reliability of quantum computation.\n",
            "\n",
            "Overall, this breakthrough has the potential to substantially impact the development and practical implementation of quantum error correction methods, a central challenge in realizing robust, fault-tolerant quantum computing.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'search_query': 'quantum error correction AND breakthroughs AND advancements AND recent',\n",
              " 'selected_paper': {'id': '2309.16104v2',\n",
              "  'title': 'Geometrically Local Quantum and Classical Codes from Subdivision',\n",
              "  'authors': ['Ting-Chun Lin', 'Adam Wills', 'Min-Hsiu Hsieh'],\n",
              "  'url': 'http://arxiv.org/pdf/2309.16104v2'},\n",
              " 'analysis': 'This paper presents a significant breakthrough in quantum error correction by developing a new construction for geometrically local quantum codes that simultaneously achieves optimal code parameters and an optimal energy barrier. Here are the key points:\\n\\n1. Methodologies:\\n\\u2003• The authors focus on balanced product codes—a specific construction of quantum low-density parity-check (qLDPC) codes—which naturally possess a two-dimensional structure.\\n\\u2003• By leveraging this inherent 2D structure, they bypass a previously necessary, mathematically intricate step (lifting a chain complex to a manifold) used in earlier constructions.\\n\\u2003• A new embedding result (detailed elsewhere) is used to embed these codes into RD, thus effectively realizing geometrically local codes in higher dimensions (D\\u2009≥\\u20093).\\n\\u2003• Their approach not only simplifies the construction but also makes it more explicit, facilitating further analysis and potential implementation.\\n\\n2. Findings:\\n\\u2003• The new code construction achieves the optimal tradeoff between code dimension and distance, matching the upper bound up to polylogarithmic factors.\\n\\u2003• In addition, the construction also attains an optimal energy barrier, a critical property for practical quantum error correction as it underpins the stability against noise.\\n\\u2003• These results extend also to certain classical code constructions, indicating a broader relevance for error correcting codes in both quantum and classical domains.\\n\\n3. Implications for Quantum Error Correction:\\n\\u2003• The paper directly addresses the question of breakthroughs in quantum error correction by presenting a method that not only meets but optimally balances the challenges of geometrical locality (necessary for short-range interactions in physical systems) with high error-correction performance.\\n\\u2003• The work builds on earlier breakthroughs (such as Portnoy’s construction) but improves upon them by streamlining the process, thus potentially reducing complexity in real-world implementations.\\n\\u2003• This advancement is particularly important because it paves the way for scalable quantum computing architectures where qubit interactions are necessarily local due to physical constraints.\\n\\u2003• Moreover, achieving an optimal energy barrier suggests that these codes could be more resilient in the face of noise, bolstering the reliability of quantum computation.\\n\\nIn summary, by simplifying the construction of geometrically local quantum codes through the innovative use of balanced product codes, the paper offers a breakthrough that could substantially impact the development and practical implementation of quantum error correction methods, addressing a central challenge in realizing robust, fault-tolerant quantum computing.',\n",
              " 'response': 'The latest breakthroughs in quantum error correction involve the development of a new construction for geometrically local quantum codes that achieves optimal code parameters and an optimal energy barrier. This is a significant advancement because it simplifies the construction process, making it more feasible for practical implementation.\\n\\nThe breakthrough is based on the use of \"balanced product codes,\" a type of quantum low-density parity-check (qLDPC) code with a natural 2D structure. By leveraging this structure, the researchers bypassed a previously necessary complex mathematical step, making the construction more explicit and easier to analyze.\\n\\nThe new code construction achieves two key milestones:\\n\\n1. **Optimal code parameters**: It strikes the optimal balance between code dimension and distance, meeting the upper bound up to polylogarithmic factors.\\n2. **Optimal energy barrier**: It attains an optimal energy barrier, a crucial property for practical quantum error correction, as it ensures stability against noise.\\n\\nThese results have far-reaching implications for both quantum and classical error-correcting codes. The breakthrough is particularly significant for quantum computing, as it:\\n\\n* **Enables scalable quantum computing architectures**: By achieving geometrical locality, the construction facilitates short-range interactions in physical systems, a necessary condition for scalable quantum computing.\\n* **Enhances resilience to noise**: The optimal energy barrier suggests that these codes could be more reliable in the face of noise, bolstering the reliability of quantum computation.\\n\\nOverall, this breakthrough has the potential to substantially impact the development and practical implementation of quantum error correction methods, a central challenge in realizing robust, fault-tolerant quantum computing.'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KaCoumGbjWws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Routing\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1e_Sq2alvr7rn10iwEKshtQ4E78435pCH)\n",
        "\n",
        "Routing is a pattern where the agent analyzes the query and directs it to the appropriate specialized pathway.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vbsouuK6q3LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RouterAgent:\n",
        "    \"\"\"\n",
        "    A router that analyzes the query and directs it to the appropriate specialized pathway.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_api_key):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.router_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a query classifier that determines the type of research question.\n",
        "Analyze the user's research question and classify it into EXACTLY ONE of the following categories:\n",
        "1. \"latest_developments\" - Questions about recent advancements or state-of-the-art in a field\n",
        "2. \"deep_dive\" - Questions requiring in-depth analysis of specific techniques or methodologies\n",
        "3. \"comparison\" - Questions comparing different approaches, methods, or theories\n",
        "4. \"application\" - Questions about applying research to specific domains or problems\n",
        "\n",
        "Respond ONLY with the category name in lowercase, nothing else.\"\"\"),\n",
        "            (\"human\", \"{query}\")\n",
        "        ])\n",
        "\n",
        "    def route(self, query):\n",
        "        \"\"\"Determine which specialized pathway should handle this query.\"\"\"\n",
        "        response = self.llm.invoke(self.router_prompt.format(query=query))\n",
        "        query_type = response.content.strip().lower()\n",
        "\n",
        "        # Validate that the response is one of our expected categories\n",
        "        valid_types = [\"latest_developments\", \"deep_dive\", \"comparison\", \"application\"]\n",
        "        if query_type not in valid_types:\n",
        "            # Default to latest_developments if classification fails\n",
        "            query_type = \"latest_developments\"\n",
        "\n",
        "        return query_type\n",
        "\n",
        "class LatestDevelopmentsAgent:\n",
        "    \"\"\"Specialized agent for handling queries about recent developments.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in the latest developments and\n",
        "state-of-the-art research. Your goal is to find the most recent papers on a topic and\n",
        "summarize the cutting-edge advancements.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class DeepDiveAgent:\n",
        "    \"\"\"Specialized agent for handling deep-dive queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in deep technical analysis.\n",
        "Your goal is to find and thoroughly analyze papers on specific techniques or methodologies.\n",
        "Focus on understanding the core principles, implementation details, and theoretical foundations.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class ComparisonAgent:\n",
        "    \"\"\"Specialized agent for handling comparison queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in comparative analysis.\n",
        "Your goal is to find papers that compare different approaches, methods, or theories and\n",
        "analyze the strengths and weaknesses of each. Create a balanced assessment that highlights\n",
        "the key differences.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class ApplicationAgent:\n",
        "    \"\"\"Specialized agent for handling application queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in practical applications.\n",
        "Your goal is to find papers that demonstrate how research is applied to real-world problems\n",
        "and domains. Focus on implementation details, case studies, and practical outcomes.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "def setup_routing_system(openai_api_key, groq_api_key):\n",
        "    \"\"\"Set up the complete routing system with specialized agents.\"\"\"\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # Create the router and specialized agents\n",
        "    router = RouterAgent(openai_api_key)\n",
        "    agents = {\n",
        "        \"latest_developments\": LatestDevelopmentsAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"deep_dive\": DeepDiveAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"comparison\": ComparisonAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"application\": ApplicationAgent(openai_api_key, groq_api_key, tools)\n",
        "    }\n",
        "\n",
        "    return router, agents\n"
      ],
      "metadata": {
        "id": "sDlEGLTJq98X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_routing_system(query, openai_api_key, groq_api_key):\n",
        "    \"\"\"Run the routing system end-to-end.\"\"\"\n",
        "    router, agents = setup_routing_system(openai_api_key, groq_api_key)\n",
        "\n",
        "    print(\"🚦 ROUTING SYSTEM INITIATED\")\n",
        "    print(f\"📋 Query: {query}\")\n",
        "\n",
        "    # Step 1: Route the query\n",
        "    print(\"\\n🔀 Routing query...\")\n",
        "    query_type = router.route(query)\n",
        "    print(f\"🔀 Query classified as: {query_type}\")\n",
        "\n",
        "    # Step 2: Process with the appropriate agent\n",
        "    print(f\"\\n🔬 Processing with specialized {query_type} agent...\")\n",
        "    response = agents[query_type].process(query)\n",
        "\n",
        "    print(\"\\n✅ ROUTING COMPLETE\")\n",
        "\n",
        "    return {\n",
        "        \"query_type\": query_type,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Example queries for different types\n",
        "    example_queries = {\n",
        "        \"latest_developments\": \"What are the latest advancements in quantum computing?\",\n",
        "        \"deep_dive\": \"How do transformers handle attention mechanisms in deep learning?\",\n",
        "        \"comparison\": \"Compare LSTM and transformer approaches for sequence modeling\",\n",
        "        \"application\": \"How is reinforcement learning applied to robotics?\"\n",
        "    }\n",
        "\n",
        "    # Choose one query to run\n",
        "    query = example_queries[\"deep_dive\"]\n",
        "\n",
        "    # Run the routing system\n",
        "    result = run_routing_system(query, openai_api_key, groq_api_key)\n",
        "\n",
        "    # Output the final response\n",
        "    print(\"\\n----- FINAL RESPONSE -----\")\n",
        "    print(result[\"response\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmiPXdlrYW8",
        "outputId": "65d19cea-c992-4538-b88b-2212b0aaa2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚦 ROUTING SYSTEM INITIATED\n",
            "📋 Query: How do transformers handle attention mechanisms in deep learning?\n",
            "\n",
            "🔀 Routing query...\n",
            "🔀 Query classified as: deep_dive\n",
            "\n",
            "🔬 Processing with specialized deep_dive agent...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTransformers handle attention mechanisms by using a structure known as self-attention (or scaled dot-product attention) to dynamically weigh the importance of different tokens in a sequence when computing representations. Here's a detailed, technical explanation of how this works:\n",
            "\n",
            "1. Key Idea of Self-Attention:\n",
            "   • Instead of processing tokens sequentially (like recurrent models), self-attention allows every token in the input sequence to attend to every other token. This enables the model to capture long-range dependencies efficiently.\n",
            "   • Tokens are embedded into continuous vector representations, and these embeddings are processed in parallel, making the approach highly scalable.\n",
            "\n",
            "2. Scaled Dot-Product Attention:\n",
            "   • For a given set of input tokens, the transformer first computes three sets of vectors through linear transformations: queries (Q), keys (K), and values (V). Each token’s embedding is projected into these vectors.\n",
            "   • The attention mechanism computes a score for each pair of tokens by taking the dot product of the query of one token with the key of another. These scores are then scaled by the inverse of the square root of the dimension of the key vectors (dₖ). The scaling helps mitigate issues with very small gradients when the dot products become large.\n",
            "   • Mathematically, this is expressed as:\n",
            "       Attention(Q, K, V) = softmax((QKᵀ) / √dₖ) V\n",
            "   • The softmax function converts the scores into a probability distribution, effectively weighting how much each token should contribute to the final representation.\n",
            "\n",
            "3. Multi-Head Attention:\n",
            "   • Rather than computing a single attention function, transformers use multiple attention heads. Each head performs the scaled dot-product attention independently using different learned linear projections.\n",
            "   • The outputs of these heads are then concatenated and passed through another linear transformation. This allows the model to capture different types of relationships between tokens simultaneously, as each head can focus on different parts of the sequence or different aspects of the data.\n",
            "\n",
            "4. Positional Encoding:\n",
            "   • Since the transformer architecture processes all tokens in parallel and lacks any inherent notion of order, positional encodings are added to the token embeddings. These encodings can be learned or fixed (e.g., based on sinusoidal functions) and provide the model with information about the relative or absolute position of tokens in the sequence.\n",
            "   • This allows the attention mechanism to take into account the order of the tokens while computing the relationships between them.\n",
            "\n",
            "5. Residual Connections and Layer Normalization:\n",
            "   • After the multi-head attention layer, residual (skip) connections and layer normalization are applied. This enhances gradient flow during training and stabilizes the learning process.\n",
            "   • Typically, the self-attention module is followed by a position-wise feed-forward network, which further processes the attended features before adding another residual connection and performing layer normalization.\n",
            "\n",
            "6. Encoder and Decoder Attention:\n",
            "   • In transformer architectures like the original “Attention is All You Need” model, the encoder employs self-attention to process the input sequence.\n",
            "   • The decoder is slightly more complex. It uses masked self-attention (to prevent attending to subsequent tokens during training for autoregressive generation) and an additional encoder-decoder attention layer. The latter allows tokens in the decoder to attend to relevant information from the encoder's output, thereby conditioning the output on the input sequence.\n",
            "\n",
            "In summary, transformers leverage the self-attention mechanism to simultaneously capture global dependencies across all tokens in a sequence. The scaled dot-product attention computes a weighted sum of all token representations, with the weights reflecting the strength of their relationships. Multi-head attention extends this by allowing the model to capture a richer set of relationships, thereby setting the foundation for the transformer’s effectiveness in a wide range of tasks in deep learning.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "✅ ROUTING COMPLETE\n",
            "\n",
            "----- FINAL RESPONSE -----\n",
            "Transformers handle attention mechanisms by using a structure known as self-attention (or scaled dot-product attention) to dynamically weigh the importance of different tokens in a sequence when computing representations. Here's a detailed, technical explanation of how this works:\n",
            "\n",
            "1. Key Idea of Self-Attention:\n",
            "   • Instead of processing tokens sequentially (like recurrent models), self-attention allows every token in the input sequence to attend to every other token. This enables the model to capture long-range dependencies efficiently.\n",
            "   • Tokens are embedded into continuous vector representations, and these embeddings are processed in parallel, making the approach highly scalable.\n",
            "\n",
            "2. Scaled Dot-Product Attention:\n",
            "   • For a given set of input tokens, the transformer first computes three sets of vectors through linear transformations: queries (Q), keys (K), and values (V). Each token’s embedding is projected into these vectors.\n",
            "   • The attention mechanism computes a score for each pair of tokens by taking the dot product of the query of one token with the key of another. These scores are then scaled by the inverse of the square root of the dimension of the key vectors (dₖ). The scaling helps mitigate issues with very small gradients when the dot products become large.\n",
            "   • Mathematically, this is expressed as:\n",
            "       Attention(Q, K, V) = softmax((QKᵀ) / √dₖ) V\n",
            "   • The softmax function converts the scores into a probability distribution, effectively weighting how much each token should contribute to the final representation.\n",
            "\n",
            "3. Multi-Head Attention:\n",
            "   • Rather than computing a single attention function, transformers use multiple attention heads. Each head performs the scaled dot-product attention independently using different learned linear projections.\n",
            "   • The outputs of these heads are then concatenated and passed through another linear transformation. This allows the model to capture different types of relationships between tokens simultaneously, as each head can focus on different parts of the sequence or different aspects of the data.\n",
            "\n",
            "4. Positional Encoding:\n",
            "   • Since the transformer architecture processes all tokens in parallel and lacks any inherent notion of order, positional encodings are added to the token embeddings. These encodings can be learned or fixed (e.g., based on sinusoidal functions) and provide the model with information about the relative or absolute position of tokens in the sequence.\n",
            "   • This allows the attention mechanism to take into account the order of the tokens while computing the relationships between them.\n",
            "\n",
            "5. Residual Connections and Layer Normalization:\n",
            "   • After the multi-head attention layer, residual (skip) connections and layer normalization are applied. This enhances gradient flow during training and stabilizes the learning process.\n",
            "   • Typically, the self-attention module is followed by a position-wise feed-forward network, which further processes the attended features before adding another residual connection and performing layer normalization.\n",
            "\n",
            "6. Encoder and Decoder Attention:\n",
            "   • In transformer architectures like the original “Attention is All You Need” model, the encoder employs self-attention to process the input sequence.\n",
            "   • The decoder is slightly more complex. It uses masked self-attention (to prevent attending to subsequent tokens during training for autoregressive generation) and an additional encoder-decoder attention layer. The latter allows tokens in the decoder to attend to relevant information from the encoder's output, thereby conditioning the output on the input sequence.\n",
            "\n",
            "In summary, transformers leverage the self-attention mechanism to simultaneously capture global dependencies across all tokens in a sequence. The scaled dot-product attention computes a weighted sum of all token representations, with the weights reflecting the strength of their relationships. Multi-head attention extends this by allowing the model to capture a richer set of relationships, thereby setting the foundation for the transformer’s effectiveness in a wide range of tasks in deep learning.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query_type': 'deep_dive',\n",
              " 'response': \"Transformers handle attention mechanisms by using a structure known as self-attention (or scaled dot-product attention) to dynamically weigh the importance of different tokens in a sequence when computing representations. Here's a detailed, technical explanation of how this works:\\n\\n1. Key Idea of Self-Attention:\\n   • Instead of processing tokens sequentially (like recurrent models), self-attention allows every token in the input sequence to attend to every other token. This enables the model to capture long-range dependencies efficiently.\\n   • Tokens are embedded into continuous vector representations, and these embeddings are processed in parallel, making the approach highly scalable.\\n\\n2. Scaled Dot-Product Attention:\\n   • For a given set of input tokens, the transformer first computes three sets of vectors through linear transformations: queries (Q), keys (K), and values (V). Each token’s embedding is projected into these vectors.\\n   • The attention mechanism computes a score for each pair of tokens by taking the dot product of the query of one token with the key of another. These scores are then scaled by the inverse of the square root of the dimension of the key vectors (dₖ). The scaling helps mitigate issues with very small gradients when the dot products become large.\\n   • Mathematically, this is expressed as:\\n       Attention(Q, K, V) = softmax((QKᵀ) / √dₖ) V\\n   • The softmax function converts the scores into a probability distribution, effectively weighting how much each token should contribute to the final representation.\\n\\n3. Multi-Head Attention:\\n   • Rather than computing a single attention function, transformers use multiple attention heads. Each head performs the scaled dot-product attention independently using different learned linear projections.\\n   • The outputs of these heads are then concatenated and passed through another linear transformation. This allows the model to capture different types of relationships between tokens simultaneously, as each head can focus on different parts of the sequence or different aspects of the data.\\n\\n4. Positional Encoding:\\n   • Since the transformer architecture processes all tokens in parallel and lacks any inherent notion of order, positional encodings are added to the token embeddings. These encodings can be learned or fixed (e.g., based on sinusoidal functions) and provide the model with information about the relative or absolute position of tokens in the sequence.\\n   • This allows the attention mechanism to take into account the order of the tokens while computing the relationships between them.\\n\\n5. Residual Connections and Layer Normalization:\\n   • After the multi-head attention layer, residual (skip) connections and layer normalization are applied. This enhances gradient flow during training and stabilizes the learning process.\\n   • Typically, the self-attention module is followed by a position-wise feed-forward network, which further processes the attended features before adding another residual connection and performing layer normalization.\\n\\n6. Encoder and Decoder Attention:\\n   • In transformer architectures like the original “Attention is All You Need” model, the encoder employs self-attention to process the input sequence.\\n   • The decoder is slightly more complex. It uses masked self-attention (to prevent attending to subsequent tokens during training for autoregressive generation) and an additional encoder-decoder attention layer. The latter allows tokens in the decoder to attend to relevant information from the encoder's output, thereby conditioning the output on the input sequence.\\n\\nIn summary, transformers leverage the self-attention mechanism to simultaneously capture global dependencies across all tokens in a sequence. The scaled dot-product attention computes a weighted sum of all token representations, with the weights reflecting the strength of their relationships. Multi-head attention extends this by allowing the model to capture a richer set of relationships, thereby setting the foundation for the transformer’s effectiveness in a wide range of tasks in deep learning.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Agent\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1_ERsFpGuxGUq_lkfuoW0O5dc9nrSXoSy)\n",
        "\n",
        "Most flexible but difficult to get it right pattern, where you are relying on LLM (usually more than one) to provide a reasoned plan of action, observe and evaluate the results of the actions from environment, reason and decide on next steps including stopping the computation.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uIP5opPY0pTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM as Orchstrator & Evaluator"
      ],
      "metadata": {
        "id": "Y7BYczEQ1Z5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "import json\n",
        "\n",
        "class AgentAction(Enum):\n",
        "    \"\"\"Enum for different possible agent actions.\"\"\"\n",
        "    USE_TOOL = \"use_tool\"\n",
        "    ROUTE = \"route\"\n",
        "    FINAL_ANSWER = \"final_answer\"\n",
        "\n",
        "class ResearchPath(Enum):\n",
        "    \"\"\"Enum for different research paths.\"\"\"\n",
        "    OVERVIEW = \"overview\"\n",
        "    DEEP_DIVE = \"deep_dive\"\n",
        "    COMPARISON = \"comparison\"\n",
        "    TECHNICAL = \"technical\"\n",
        "\n",
        "# Define the schema for LLM response using Pydantic\n",
        "class ToolActionInput(BaseModel):\n",
        "    \"\"\"Schema for tool action input.\"\"\"\n",
        "    tool_name: str = Field(description=\"Name of the tool to use\")\n",
        "    tool_input: str = Field(description=\"Input to provide to the tool\")\n",
        "\n",
        "class RouteActionInput(BaseModel):\n",
        "    \"\"\"Schema for routing action input.\"\"\"\n",
        "    path: str = Field(description=\"Research path to take\")\n",
        "\n",
        "class FinalAnswerInput(BaseModel):\n",
        "    \"\"\"Schema for final answer input.\"\"\"\n",
        "    answer: str = Field(description=\"Final answer to the query\")\n",
        "\n",
        "class AgentDecision(BaseModel):\n",
        "    \"\"\"Schema for agent decision.\"\"\"\n",
        "    thought: str = Field(description=\"Detailed reasoning about the current state and what to do next\")\n",
        "    action: str = Field(description=\"Action to take: use_tool, route, or final_answer\")\n",
        "    action_input: Union[ToolActionInput, RouteActionInput, FinalAnswerInput] = Field(\n",
        "        description=\"Input for the action\"\n",
        "    )\n",
        "\n",
        "class AgentState:\n",
        "    \"\"\"State container for the agent's reasoning process.\"\"\"\n",
        "    def __init__(self, query: str):\n",
        "        self.query = query\n",
        "        self.current_path: Optional[ResearchPath] = None\n",
        "        self.tool_results: List[Dict[str, Any]] = []\n",
        "        self.working_memory: List[str] = []\n",
        "        self.final_answer: Optional[str] = None\n",
        "        self.conversation_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    def add_tool_result(self, tool_name: str, tool_input: str, tool_output: Any) -> None:\n",
        "        \"\"\"Add the result of a tool call to the state.\"\"\"\n",
        "        self.tool_results.append({\n",
        "            \"tool_name\": tool_name,\n",
        "            \"tool_input\": tool_input,\n",
        "            \"tool_output\": tool_output\n",
        "        })\n",
        "\n",
        "    def add_to_memory(self, thought: str) -> None:\n",
        "        \"\"\"Add a thought to working memory.\"\"\"\n",
        "        self.working_memory.append(thought)\n",
        "\n",
        "    def set_path(self, path: ResearchPath) -> None:\n",
        "        \"\"\"Set the current research path.\"\"\"\n",
        "        self.current_path = path\n",
        "\n",
        "    def set_final_answer(self, answer: str) -> None:\n",
        "        \"\"\"Set the final answer.\"\"\"\n",
        "        self.final_answer = answer\n",
        "\n",
        "    def add_message(self, role: str, content: str) -> None:\n",
        "        \"\"\"Add a message to the conversation history.\"\"\"\n",
        "        self.conversation_history.append({\n",
        "            \"role\": role,\n",
        "            \"content\": content\n",
        "        })\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert the state to a dictionary for serialization.\"\"\"\n",
        "        return {\n",
        "            \"query\": self.query,\n",
        "            \"current_path\": self.current_path.value if self.current_path else None,\n",
        "            \"tool_results\": self.tool_results,\n",
        "            \"working_memory\": self.working_memory,\n",
        "            \"final_answer\": self.final_answer,\n",
        "        }\n",
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"An agent that can dynamically make decisions about research queries.\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key: str, groq_api_key: str):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\", temperature=0)\n",
        "        self.groq_api_key = groq_api_key\n",
        "        self.tools = create_tools(groq_api_key)\n",
        "\n",
        "        # Create the JSON output parser\n",
        "        self.parser = JsonOutputParser(pydantic_object=AgentDecision)\n",
        "\n",
        "        # Create the decision prompt template with LangChain's JSON parser format instructions\n",
        "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant agent capable of dynamically making decisions about how to answer research questions.\n",
        "You must explicitly reason through THREE types of decisions:\n",
        "\n",
        "1. ROUTING - Determine which research path would best address the query:\n",
        "   - OVERVIEW: For broad, introductory questions needing a general summary\n",
        "   - DEEP_DIVE: For questions requiring in-depth focus on a specific topic\n",
        "   - COMPARISON: For questions comparing multiple approaches or methods\n",
        "   - TECHNICAL: For questions requiring detailed technical explanations\n",
        "\n",
        "2. TOOL SELECTION - Determine which tool to use next from:\n",
        "   - search_arxiv: Search for academic papers (input: search query)\n",
        "   - download_paper: Get the text of a specific paper (input: paper ID)\n",
        "   - summarize_text: Summarize a piece of text (input: text to summarize)\n",
        "\n",
        "3. FINAL ANSWER - Determine if you have enough information to provide a final answer\n",
        "\n",
        "For each decision, you MUST use the following process:\n",
        "1. Think through the options step-by-step\n",
        "2. Consider the query and current state\n",
        "3. Make an explicit choice\n",
        "4. Explain your reasoning\n",
        "\n",
        "{format_instructions}\"\"\"),\n",
        "            (\"human\", \"\"\"\n",
        "QUERY: {query}\n",
        "\n",
        "CURRENT PATH: {current_path}\n",
        "\n",
        "TOOL RESULTS:\n",
        "{tool_results}\n",
        "\n",
        "WORKING MEMORY:\n",
        "{working_memory}\n",
        "\n",
        "Based on the above, what should I do next? Think through whether I should choose a research path, use a tool, or provide a final answer.\n",
        "\"\"\")\n",
        "        ])\n",
        "\n",
        "    def _format_tool_results(self, tool_results: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format tool results for the prompt.\"\"\"\n",
        "        if not tool_results:\n",
        "            return \"No tools have been used yet.\"\n",
        "\n",
        "        result_strings = []\n",
        "        for i, result in enumerate(tool_results):\n",
        "            result_str = f\"Tool {i+1}: {result['tool_name']}\\n\"\n",
        "            result_str += f\"Input: {result['tool_input']}\\n\"\n",
        "\n",
        "            # Format the output depending on the tool\n",
        "            if result['tool_name'] == \"search_arxiv\":\n",
        "                papers = result['tool_output']\n",
        "                result_str += \"Output (Papers):\\n\"\n",
        "                for j, paper in enumerate(papers):\n",
        "                    result_str += f\"  Paper {j+1}: {paper['title']}\\n\"\n",
        "                    result_str += f\"  ID: {paper['id']}\\n\"\n",
        "                    result_str += f\"  Authors: {', '.join(paper['authors'])}\\n\"\n",
        "                    result_str += f\"  URL: {paper['url']}\\n\\n\"\n",
        "            elif result['tool_name'] == \"download_paper\":\n",
        "                # Truncate long paper texts\n",
        "                paper_text = result['tool_output']\n",
        "                if len(paper_text) > 500:\n",
        "                    paper_text = paper_text[:500] + \"... [text truncated]\"\n",
        "                result_str += f\"Output (Paper Text):\\n{paper_text}\\n\\n\"\n",
        "            elif result['tool_name'] == \"summarize_text\":\n",
        "                result_str += f\"Output (Summary):\\n{result['tool_output']}\\n\\n\"\n",
        "\n",
        "            result_strings.append(result_str)\n",
        "\n",
        "        return \"\\n\".join(result_strings)\n",
        "\n",
        "    def _format_working_memory(self, working_memory: List[str]) -> str:\n",
        "        \"\"\"Format working memory for the prompt.\"\"\"\n",
        "        if not working_memory:\n",
        "            return \"No thoughts recorded yet.\"\n",
        "\n",
        "        memory_strings = []\n",
        "        for i, thought in enumerate(working_memory):\n",
        "            memory_strings.append(f\"Thought {i+1}: {thought}\")\n",
        "\n",
        "        return \"\\n\".join(memory_strings)\n",
        "\n",
        "    def _get_next_action(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Get the next action from the LLM.\"\"\"\n",
        "        # Format the state for the prompt\n",
        "        prompt_args = {\n",
        "            \"query\": state.query,\n",
        "            \"current_path\": state.current_path.value if state.current_path else \"Not selected yet\",\n",
        "            \"tool_results\": self._format_tool_results(state.tool_results),\n",
        "            \"working_memory\": self._format_working_memory(state.working_memory),\n",
        "            \"format_instructions\": self.parser.get_format_instructions()\n",
        "        }\n",
        "\n",
        "        # Get the LLM's decision\n",
        "        response = self.llm.invoke(self.decision_prompt.format(**prompt_args))\n",
        "\n",
        "        # Parse the JSON response using LangChain's JSON parser\n",
        "        try:\n",
        "            action_data = self.parser.parse(response.content)\n",
        "\n",
        "            # Convert the Pydantic model to a dictionary\n",
        "            result = {\n",
        "                \"thought\": action_data.thought,\n",
        "                \"action\": action_data.action,\n",
        "                \"action_input\": {}\n",
        "            }\n",
        "\n",
        "            # Extract the appropriate action input fields\n",
        "            if action_data.action == AgentAction.USE_TOOL.value:\n",
        "                action_input = action_data.action_input\n",
        "                result[\"action_input\"] = {\n",
        "                    \"tool_name\": action_input.tool_name,\n",
        "                    \"tool_input\": action_input.tool_input\n",
        "                }\n",
        "            elif action_data.action == AgentAction.ROUTE.value:\n",
        "                action_input = action_data.action_input\n",
        "                result[\"action_input\"] = {\n",
        "                    \"path\": action_input.path\n",
        "                }\n",
        "            elif action_data.action == AgentAction.FINAL_ANSWER.value:\n",
        "                action_input = action_data.action_input\n",
        "                result[\"action_input\"] = {\n",
        "                    \"answer\": action_input.answer\n",
        "                }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Better error handling\n",
        "            print(f\"Error parsing LLM response: {e}\")\n",
        "            print(f\"Problem response: {response.content}\")\n",
        "\n",
        "            # Fall back to a predetermined structure\n",
        "            return {\n",
        "                \"thought\": f\"There was an error parsing my previous response: {str(e)}\",\n",
        "                \"action\": \"final_answer\",\n",
        "                \"action_input\": {\n",
        "                    \"answer\": \"I encountered a technical error in my reasoning process. Let me provide a direct answer based on my knowledge: Transformers and LSTMs differ primarily in how they process sequential data - LSTMs process data sequentially while transformers can process the entire sequence at once through self-attention mechanisms.\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def _execute_tool(self, tool_name: str, tool_input: str) -> Any:\n",
        "        \"\"\"Execute a tool and return the result.\"\"\"\n",
        "        # Find the tool\n",
        "        tool = next((t for t in self.tools if t.name == tool_name), None)\n",
        "        if not tool:\n",
        "            return f\"Error: Tool '{tool_name}' not found.\"\n",
        "\n",
        "        # Execute the tool\n",
        "        try:\n",
        "            result = tool.func(tool_input)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return f\"Error executing tool: {str(e)}\"\n",
        "\n",
        "    def _route_query(self, path_name: str) -> ResearchPath:\n",
        "        \"\"\"Route the query to a research path.\"\"\"\n",
        "        try:\n",
        "            return ResearchPath(path_name)\n",
        "        except ValueError:\n",
        "            # Default to overview if invalid path\n",
        "            return ResearchPath.OVERVIEW\n",
        "\n",
        "    def process_query(self, query: str, max_steps: int = 10) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a research query using the agent's reasoning loop.\n",
        "\n",
        "        Args:\n",
        "            query: The research query to process\n",
        "            max_steps: Maximum number of reasoning steps to perform\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final answer and the agent's state\n",
        "        \"\"\"\n",
        "        # Initialize the agent state\n",
        "        state = AgentState(query)\n",
        "\n",
        "        print(\"🤖 RESEARCH AGENT INITIATED\")\n",
        "        print(f\"📋 Query: {query}\")\n",
        "\n",
        "        # Main reasoning loop\n",
        "        for step in range(max_steps):\n",
        "            print(f\"\\n🔄 Step {step+1}: Reasoning...\")\n",
        "\n",
        "            # Get the next action\n",
        "            action_data = self._get_next_action(state)\n",
        "\n",
        "            # Add the thought to working memory\n",
        "            thought = action_data.get(\"thought\", \"No reasoning provided\")\n",
        "            state.add_to_memory(thought)\n",
        "            print(f\"💭 Thought: {thought}\")\n",
        "\n",
        "            # Parse the action\n",
        "            action = action_data.get(\"action\")\n",
        "            action_input = action_data.get(\"action_input\", {})\n",
        "\n",
        "            if action == AgentAction.USE_TOOL.value:\n",
        "                # Execute the tool\n",
        "                tool_name = action_input.get(\"tool_name\")\n",
        "                tool_input = action_input.get(\"tool_input\")\n",
        "\n",
        "                print(f\"🔧 Using Tool: {tool_name}\")\n",
        "                print(f\"📥 Tool Input: {tool_input}\")\n",
        "\n",
        "                tool_output = self._execute_tool(tool_name, tool_input)\n",
        "                state.add_tool_result(tool_name, tool_input, tool_output)\n",
        "\n",
        "                print(f\"📤 Tool Output: {type(tool_output)} with {len(str(tool_output))} chars\")\n",
        "\n",
        "            elif action == AgentAction.ROUTE.value:\n",
        "                # Set the research path\n",
        "                path_name = action_input.get(\"path\")\n",
        "                path = self._route_query(path_name)\n",
        "                state.set_path(path)\n",
        "\n",
        "                print(f\"🔀 Routing to Path: {path.value}\")\n",
        "\n",
        "            elif action == AgentAction.FINAL_ANSWER.value:\n",
        "                # Set the final answer\n",
        "                answer = action_input.get(\"answer\")\n",
        "                state.set_final_answer(answer)\n",
        "\n",
        "                print(f\"✅ Final Answer Ready\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                # Handle unknown action\n",
        "                print(f\"⚠️ Unknown Action: {action}\")\n",
        "                state.set_final_answer(f\"I encountered an error in my reasoning process. Unknown action: {action}\")\n",
        "                break\n",
        "        else:\n",
        "            # Handle max steps reached\n",
        "            state.set_final_answer(\"I've reached the maximum number of reasoning steps without finding a complete answer. Here's what I've learned so far: \" +\n",
        "                                 \"\\n\\n\".join(state.working_memory[-3:]))\n",
        "            print(\"⚠️ Maximum steps reached without final answer\")\n",
        "\n",
        "        print(\"\\n✨ REASONING COMPLETE\")\n",
        "\n",
        "        return {\n",
        "            \"final_answer\": state.final_answer,\n",
        "            \"state\": state.to_dict()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "bHOer1VU00a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Create the agent\n",
        "    agent = ResearchAgent(openai_api_key, groq_api_key)\n",
        "\n",
        "    # Example query\n",
        "    query = \"What are the differences between transformers and LSTMs for NLP tasks?\"\n",
        "\n",
        "    # Process the query\n",
        "    result = agent.process_query(query)\n",
        "\n",
        "    # Output the final answer\n",
        "    print(\"\\n----- FINAL ANSWER -----\")\n",
        "    print(result[\"final_answer\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ],
      "metadata": {
        "id": "vnaJ5DyJ2VPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa3baee-b97f-476f-afee-3b4e513ade80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 RESEARCH AGENT INITIATED\n",
            "📋 Query: What are the differences between transformers and LSTMs for NLP tasks?\n",
            "\n",
            "🔄 Step 1: Reasoning...\n",
            "input_variables=['current_path', 'query', 'tool_results', 'working_memory'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a research assistant agent capable of dynamically making decisions about how to answer research questions.\\nYou must explicitly reason through THREE types of decisions:\\n\\n1. ROUTING - Determine which research path would best address the query:\\n   - OVERVIEW: For broad, introductory questions needing a general summary\\n   - DEEP_DIVE: For questions requiring in-depth focus on a specific topic\\n   - COMPARISON: For questions comparing multiple approaches or methods\\n   - TECHNICAL: For questions requiring detailed technical explanations\\n\\n2. TOOL SELECTION - Determine which tool to use next from:\\n   - search_arxiv: Search for academic papers (input: search query)\\n   - download_paper: Get the text of a specific paper (input: paper ID)\\n   - summarize_text: Summarize a piece of text (input: text to summarize)\\n\\n3. FINAL ANSWER - Determine if you have enough information to provide a final answer\\n\\nFor each decision, you MUST use the following process:\\n1. Think through the options step-by-step\\n2. Consider the query and current state\\n3. Make an explicit choice\\n4. Explain your reasoning\\n\\nYour response MUST be in this JSON format:\\n{{\\n  \"thought\": \"Your detailed reasoning about the current state and what to do next\",\\n  \"action\": \"ONE OF: use_tool, route, final_answer\",\\n  \"action_input\": {{\\n    \"tool_name\": \"name of the tool to use (if action is use_tool)\",\\n    \"tool_input\": \"input to the tool (if action is use_tool)\",\\n    \"path\": \"research path to take (if action is route)\",\\n    \"answer\": \"final answer to the query (if action is final_answer)\"\\n  }}\\n}}\\n\\n\\nONLY include the keys needed for your chosen action. Your JSON must be perfectly formatted with no errors and must be the ONLY content in your response.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['current_path', 'query', 'tool_results', 'working_memory'], input_types={}, partial_variables={}, template='\\nQUERY: {query}\\n\\nCURRENT PATH: {current_path}\\n\\nTOOL RESULTS:\\n{tool_results}\\n\\nWORKING MEMORY:\\n{working_memory}\\n\\nBased on the above, what should I do next? Think through whether I should choose a research path, use a tool, or provide a final answer.\\n'), additional_kwargs={})]\n",
            "{'query': 'What are the differences between transformers and LSTMs for NLP tasks?', 'current_path': 'Not selected yet', 'tool_results': 'No tools have been used yet.', 'working_memory': 'No thoughts recorded yet.'}\n",
            "💭 Thought: The query asks for a comparison between transformers and LSTMs for NLP tasks, which directly points to a COMPARISON research path.\n",
            "🔀 Routing to Path: overview\n",
            "\n",
            "🔄 Step 2: Reasoning...\n",
            "input_variables=['current_path', 'query', 'tool_results', 'working_memory'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a research assistant agent capable of dynamically making decisions about how to answer research questions.\\nYou must explicitly reason through THREE types of decisions:\\n\\n1. ROUTING - Determine which research path would best address the query:\\n   - OVERVIEW: For broad, introductory questions needing a general summary\\n   - DEEP_DIVE: For questions requiring in-depth focus on a specific topic\\n   - COMPARISON: For questions comparing multiple approaches or methods\\n   - TECHNICAL: For questions requiring detailed technical explanations\\n\\n2. TOOL SELECTION - Determine which tool to use next from:\\n   - search_arxiv: Search for academic papers (input: search query)\\n   - download_paper: Get the text of a specific paper (input: paper ID)\\n   - summarize_text: Summarize a piece of text (input: text to summarize)\\n\\n3. FINAL ANSWER - Determine if you have enough information to provide a final answer\\n\\nFor each decision, you MUST use the following process:\\n1. Think through the options step-by-step\\n2. Consider the query and current state\\n3. Make an explicit choice\\n4. Explain your reasoning\\n\\nYour response MUST be in this JSON format:\\n{{\\n  \"thought\": \"Your detailed reasoning about the current state and what to do next\",\\n  \"action\": \"ONE OF: use_tool, route, final_answer\",\\n  \"action_input\": {{\\n    \"tool_name\": \"name of the tool to use (if action is use_tool)\",\\n    \"tool_input\": \"input to the tool (if action is use_tool)\",\\n    \"path\": \"research path to take (if action is route)\",\\n    \"answer\": \"final answer to the query (if action is final_answer)\"\\n  }}\\n}}\\n\\n\\nONLY include the keys needed for your chosen action. Your JSON must be perfectly formatted with no errors and must be the ONLY content in your response.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['current_path', 'query', 'tool_results', 'working_memory'], input_types={}, partial_variables={}, template='\\nQUERY: {query}\\n\\nCURRENT PATH: {current_path}\\n\\nTOOL RESULTS:\\n{tool_results}\\n\\nWORKING MEMORY:\\n{working_memory}\\n\\nBased on the above, what should I do next? Think through whether I should choose a research path, use a tool, or provide a final answer.\\n'), additional_kwargs={})]\n",
            "{'query': 'What are the differences between transformers and LSTMs for NLP tasks?', 'current_path': 'overview', 'tool_results': 'No tools have been used yet.', 'working_memory': 'Thought 1: The query asks for a comparison between transformers and LSTMs for NLP tasks, which directly points to a COMPARISON research path.'}\n",
            "💭 Thought: The query asks for a comparison between transformers and LSTMs for NLP tasks. This requires detailed analysis of their structural differences, performance, training considerations, and applicability to various NLP tasks. The working memory suggests that the proper research path is a COMPARISON. I believe I have enough knowledge to provide a comprehensive final answer that includes the major differences between the two architectures.\n",
            "✅ Final Answer Ready\n",
            "\n",
            "✨ REASONING COMPLETE\n",
            "\n",
            "----- FINAL ANSWER -----\n",
            "Transformers and LSTMs are both popular architectures for NLP tasks but differ significantly in their design and capabilities:\n",
            "\n",
            "1. Architecture: \n",
            "   - LSTMs (Long Short-Term Memory networks) are a type of recurrent neural network (RNN) designed to handle sequential data using gates to manage and retain information over sequences. They process input sequentially, which can limit parallelization.\n",
            "   - Transformers, on the other hand, use self-attention mechanisms to weigh the importance of different parts of the input sequence simultaneously. This allows them to capture long-range dependencies more effectively and enables better parallelization during training.\n",
            "\n",
            "2. Training Efficiency:\n",
            "   - LSTMs process words one at a time, leading to slower training especially with long sequences. They are more prone to issues with vanishing or exploding gradients, which can make training challenging for very long texts.\n",
            "   - Transformers allow for parallel processing because they do not rely on sequential steps. This makes them more computationally efficient and scalable on modern hardware, resulting in faster training times.\n",
            "\n",
            "3. Learning Dependencies:\n",
            "   - LSTMs can struggle with very long-range dependencies because the sequential nature of the architecture can cause information to degrade over many time steps, even with gating mechanisms.\n",
            "   - Transformers, thanks to the self-attention mechanism, can capture relationships between all tokens in a sequence regardless of their distance. This makes them particularly effective at handling complex, long-distance dependencies in text.\n",
            "\n",
            "4. Applications and Model Scale:\n",
            "   - LSTMs have been successfully applied to various NLP tasks (e.g., language modeling, machine translation) and are still used for scenarios where sequential processing is inherent. They are usually lighter in terms of resource requirements compared to large transformer models.\n",
            "   - Transformers are the backbone of state-of-the-art models like BERT, GPT, and others. Their ability to pre-train on large corpora and fine-tune on downstream tasks has led to breakthroughs in many areas of NLP. However, they also tend to be more resource-intensive and require more data to perform optimally.\n",
            "\n",
            "5. Contextual Representations:\n",
            "   - LSTMs generate context-dependent representations step-by-step, which might not fully capture the nuances of context in all parts of a sentence, especially when handling long texts.\n",
            "   - Transformers' self-attention mechanism allows each token to attend to every other token, leading to richer context-aware representations that have proven very powerful in modern NLP tasks.\n",
            "\n",
            "In summary, while LSTMs are useful for sequential data and relatively lightweight models, transformers offer superior efficiency, scalability, and performance in capturing complex dependencies in text, which is why they have become the dominant architecture in current NLP research and applications.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'final_answer': \"Transformers and LSTMs are both popular architectures for NLP tasks but differ significantly in their design and capabilities:\\n\\n1. Architecture: \\n   - LSTMs (Long Short-Term Memory networks) are a type of recurrent neural network (RNN) designed to handle sequential data using gates to manage and retain information over sequences. They process input sequentially, which can limit parallelization.\\n   - Transformers, on the other hand, use self-attention mechanisms to weigh the importance of different parts of the input sequence simultaneously. This allows them to capture long-range dependencies more effectively and enables better parallelization during training.\\n\\n2. Training Efficiency:\\n   - LSTMs process words one at a time, leading to slower training especially with long sequences. They are more prone to issues with vanishing or exploding gradients, which can make training challenging for very long texts.\\n   - Transformers allow for parallel processing because they do not rely on sequential steps. This makes them more computationally efficient and scalable on modern hardware, resulting in faster training times.\\n\\n3. Learning Dependencies:\\n   - LSTMs can struggle with very long-range dependencies because the sequential nature of the architecture can cause information to degrade over many time steps, even with gating mechanisms.\\n   - Transformers, thanks to the self-attention mechanism, can capture relationships between all tokens in a sequence regardless of their distance. This makes them particularly effective at handling complex, long-distance dependencies in text.\\n\\n4. Applications and Model Scale:\\n   - LSTMs have been successfully applied to various NLP tasks (e.g., language modeling, machine translation) and are still used for scenarios where sequential processing is inherent. They are usually lighter in terms of resource requirements compared to large transformer models.\\n   - Transformers are the backbone of state-of-the-art models like BERT, GPT, and others. Their ability to pre-train on large corpora and fine-tune on downstream tasks has led to breakthroughs in many areas of NLP. However, they also tend to be more resource-intensive and require more data to perform optimally.\\n\\n5. Contextual Representations:\\n   - LSTMs generate context-dependent representations step-by-step, which might not fully capture the nuances of context in all parts of a sentence, especially when handling long texts.\\n   - Transformers' self-attention mechanism allows each token to attend to every other token, leading to richer context-aware representations that have proven very powerful in modern NLP tasks.\\n\\nIn summary, while LSTMs are useful for sequential data and relatively lightweight models, transformers offer superior efficiency, scalability, and performance in capturing complex dependencies in text, which is why they have become the dominant architecture in current NLP research and applications.\",\n",
              " 'state': {'query': 'What are the differences between transformers and LSTMs for NLP tasks?',\n",
              "  'current_path': 'overview',\n",
              "  'tool_results': [],\n",
              "  'working_memory': ['The query asks for a comparison between transformers and LSTMs for NLP tasks, which directly points to a COMPARISON research path.',\n",
              "   'The query asks for a comparison between transformers and LSTMs for NLP tasks. This requires detailed analysis of their structural differences, performance, training considerations, and applicability to various NLP tasks. The working memory suggests that the proper research path is a COMPARISON. I believe I have enough knowledge to provide a comprehensive final answer that includes the major differences between the two architectures.'],\n",
              "  'final_answer': \"Transformers and LSTMs are both popular architectures for NLP tasks but differ significantly in their design and capabilities:\\n\\n1. Architecture: \\n   - LSTMs (Long Short-Term Memory networks) are a type of recurrent neural network (RNN) designed to handle sequential data using gates to manage and retain information over sequences. They process input sequentially, which can limit parallelization.\\n   - Transformers, on the other hand, use self-attention mechanisms to weigh the importance of different parts of the input sequence simultaneously. This allows them to capture long-range dependencies more effectively and enables better parallelization during training.\\n\\n2. Training Efficiency:\\n   - LSTMs process words one at a time, leading to slower training especially with long sequences. They are more prone to issues with vanishing or exploding gradients, which can make training challenging for very long texts.\\n   - Transformers allow for parallel processing because they do not rely on sequential steps. This makes them more computationally efficient and scalable on modern hardware, resulting in faster training times.\\n\\n3. Learning Dependencies:\\n   - LSTMs can struggle with very long-range dependencies because the sequential nature of the architecture can cause information to degrade over many time steps, even with gating mechanisms.\\n   - Transformers, thanks to the self-attention mechanism, can capture relationships between all tokens in a sequence regardless of their distance. This makes them particularly effective at handling complex, long-distance dependencies in text.\\n\\n4. Applications and Model Scale:\\n   - LSTMs have been successfully applied to various NLP tasks (e.g., language modeling, machine translation) and are still used for scenarios where sequential processing is inherent. They are usually lighter in terms of resource requirements compared to large transformer models.\\n   - Transformers are the backbone of state-of-the-art models like BERT, GPT, and others. Their ability to pre-train on large corpora and fine-tune on downstream tasks has led to breakthroughs in many areas of NLP. However, they also tend to be more resource-intensive and require more data to perform optimally.\\n\\n5. Contextual Representations:\\n   - LSTMs generate context-dependent representations step-by-step, which might not fully capture the nuances of context in all parts of a sentence, especially when handling long texts.\\n   - Transformers' self-attention mechanism allows each token to attend to every other token, leading to richer context-aware representations that have proven very powerful in modern NLP tasks.\\n\\nIn summary, while LSTMs are useful for sequential data and relatively lightweight models, transformers offer superior efficiency, scalability, and performance in capturing complex dependencies in text, which is why they have become the dominant architecture in current NLP research and applications.\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] A Succinct Version\n",
        "\n",
        "This implementation of the agent uses LangGraph framework to make the above verbose version (which was more for illustrative perposes) more succinct while not loosing any functionality.\n",
        "\n",
        "This goes on to show that frameworks like `langgraph` abstract common boiler plate code into the frameworks and provide important prebuilt constructs like OutputParser to simplify the agent building and working with LLMs.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GOaAIXOE4_RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Define tool schemas for better structured input ----\n",
        "\n",
        "class SearchArxivInput(BaseModel):\n",
        "    \"\"\"Input schema for searching arXiv.\"\"\"\n",
        "    query: str = Field(description=\"The search query for arXiv papers\")\n",
        "    max_results: int = Field(default=5, description=\"Maximum number of results\")\n",
        "\n",
        "class DownloadPaperInput(BaseModel):\n",
        "    \"\"\"Input schema for downloading a paper.\"\"\"\n",
        "    paper_id: str = Field(description=\"The arXiv paper ID to download\")\n",
        "\n",
        "class SummarizeTextInput(BaseModel):\n",
        "    \"\"\"Input schema for text summarization.\"\"\"\n",
        "    text: str = Field(description=\"The text to summarize\")\n",
        "\n",
        "def create_structured_tools(groq_api_key: str) -> List[StructuredTool]:\n",
        "    \"\"\"Create structured tools for the agent.\"\"\"\n",
        "\n",
        "    search_arxiv_tool = StructuredTool.from_function(\n",
        "        func=search_arxiv,\n",
        "        name=\"search_arxiv\",\n",
        "        description=\"\"\"Search for academic papers on arXiv. This tool returns a list of papers\n",
        "            with their titles, IDs, authors, and URLs. Use this when you need to find papers on a topic.\"\"\",\n",
        "        args_schema=SearchArxivInput\n",
        "    )\n",
        "\n",
        "    download_paper_tool = StructuredTool.from_function(\n",
        "        func=download_paper,\n",
        "        name=\"download_paper\",\n",
        "        description=\"\"\"Download and extract the text content of an arXiv paper.\n",
        "            Provide the paper ID (just the number part). Use this after finding\n",
        "            papers with search_arxiv to read their content.\"\"\",\n",
        "        args_schema=DownloadPaperInput\n",
        "    )\n",
        "\n",
        "    summarize_text_tool = StructuredTool.from_function(\n",
        "        func=lambda text: summarize_text(text, groq_api_key),\n",
        "        name=\"summarize_text\",\n",
        "        description=\"\"\"Summarize a piece of text into key points.\n",
        "            Use this to condense long paper content into digestible summaries.\"\"\",\n",
        "        args_schema=SummarizeTextInput\n",
        "    )\n",
        "\n",
        "    return [search_arxiv_tool, download_paper_tool, summarize_text_tool]\n",
        "\n",
        "# ---- Agent Setup ----\n",
        "\n",
        "def create_research_agent(openai_api_key: str, groq_api_key: str) -> AgentExecutor:\n",
        "    \"\"\"Create a research agent using LangChain's AgentExecutor.\"\"\"\n",
        "\n",
        "    # Create the LLM\n",
        "    llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "\n",
        "    # Create the structured tools\n",
        "    tools = create_structured_tools(groq_api_key)\n",
        "\n",
        "    # Create the agent prompt\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful AI research assistant. You have access to tools for searching academic papers,\n",
        "            downloading paper content, and summarizing text.\n",
        "\n",
        "            Your approach should be:\n",
        "            1. Understand the research question\n",
        "            2. Search for relevant papers using appropriate keywords\n",
        "            3. Download and analyze the most relevant papers\n",
        "            4. Synthesize information to answer the question\n",
        "\n",
        "            Think step by step and make decisions about which tools to use when.\n",
        "            When you've gathered enough information, provide a comprehensive answer.\"\"\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "\n",
        "    # Create the agent\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # Create agent executor\n",
        "    return AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=10)\n",
        "\n",
        "# ---- Custom Agent with Decision Logging ----\n",
        "\n",
        "class ResearchAgentWithDecisions(AgentExecutor):\n",
        "    \"\"\"Extended AgentExecutor that logs decision points for educational purposes.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        object.__setattr__(self, 'decision_log', [])\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Override _call to log decision points.\"\"\"\n",
        "\n",
        "        print(\"🤖 RESEARCH AGENT INITIATED\")\n",
        "        print(f\"📋 Query: {inputs.get('input', '')}\")\n",
        "\n",
        "        # Call the parent class method\n",
        "        result = super()._call(inputs, *args, **kwargs)\n",
        "\n",
        "        print(\"\\n✨ AGENT EXECUTION COMPLETE\")\n",
        "        print(\"\\n----- DECISION LOG -----\")\n",
        "        for i, decision in enumerate(self.decision_log):\n",
        "            print(f\"{i+1}. {decision}\")\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _take_next_step(self, name_to_tool_map: Dict[str, Any], color_mapping: Dict[str, str],\n",
        "                       inputs: Dict[str, Any], intermediate_steps: List[tuple], *args, **kwargs) -> Any:\n",
        "        \"\"\"Override to log agent decisions.\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = super()._take_next_step(name_to_tool_map, color_mapping, inputs, intermediate_steps, *args, **kwargs)\n",
        "\n",
        "            # Log the decision\n",
        "            if isinstance(result, list):\n",
        "                for r in result:\n",
        "                    if isinstance(r, tuple) and isinstance(r[0], AgentAction):\n",
        "                        action = r[0]\n",
        "                        decision = f\"Decided to use tool: {action.tool} with input: {action.tool_input}\"\n",
        "                        self.decision_log.append(decision)\n",
        "                        print(f\"\\n🔄 Agent Decision: {decision}\")\n",
        "            elif isinstance(result, AgentFinish):\n",
        "                decision = f\"Decided to finish with output: {result.return_values.get('output', '')[:100]}...\"\n",
        "                self.decision_log.append(decision)\n",
        "                print(f\"\\n✅ Agent Finish: {decision}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in agent execution: {e}\")\n",
        "            return AgentFinish(return_values={\"output\": \"An error occurred during execution.\"}, log=\"Error\")\n",
        "\n",
        "# ---- Usage Example ----\n",
        "\n",
        "def run_research_query(query: str, openai_api_key: str, groq_api_key: str) -> str:\n",
        "    \"\"\"Run a research query using the agent.\"\"\"\n",
        "\n",
        "    # Create the research agent\n",
        "    agent_executor = ResearchAgentWithDecisions(\n",
        "        agent=create_openai_tools_agent(\n",
        "            ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\"),\n",
        "            create_structured_tools(groq_api_key),\n",
        "            ChatPromptTemplate.from_messages([\n",
        "                (\"system\", \"\"\"You are a helpful AI research assistant. You have access to tools for searching academic papers,\n",
        "                    downloading paper content, and summarizing text.\n",
        "\n",
        "                    Your approach should be:\n",
        "                    1. Understand the research question\n",
        "                    2. Search for relevant papers using appropriate keywords\n",
        "                    3. Download and analyze the most relevant papers\n",
        "                    4. Synthesize information to answer the question\n",
        "\n",
        "                    Think step by step and make decisions about which tools to use when.\n",
        "                    When you've gathered enough information, provide a comprehensive answer.\"\"\"),\n",
        "                (\"human\", \"{input}\"),\n",
        "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "            ])\n",
        "        ),\n",
        "        tools=create_structured_tools(groq_api_key),\n",
        "        verbose=True,\n",
        "        max_iterations=10\n",
        "    )\n",
        "\n",
        "    # Execute the query\n",
        "    result = agent_executor.invoke({\"input\": query})\n",
        "\n",
        "    return result[\"output\"]\n",
        "\n",
        "# ---- Example usage ----\n",
        "def run_example(passed_query=\"\"):\n",
        "    # Load API keys from environment\n",
        "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Example query\n",
        "    # query = \"What are the latest advancements in quantum error correction?\"\n",
        "    query = \"What some progound insights in the best papers covering fine tuning in LLMs?\"\n",
        "    if passed_query:\n",
        "        query = passed_query\n",
        "\n",
        "    # Run the query\n",
        "    response = run_research_query(query, openai_api_key, groq_api_key)\n",
        "\n",
        "    print(\"\\n----- FINAL ANSWER -----\")\n",
        "    print(response)\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "S8wXhFZB511R"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_example(passed_query=\"What some profound insights in the best papers covering fine tuning in LLMs?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0J5x_7GjkyZN",
        "outputId": "1517e102-a165-4294-9e29-013b9dc82b3f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ResearchAgentWithDecisions chain...\u001b[0m\n",
            "🤖 RESEARCH AGENT INITIATED\n",
            "📋 Query: What some progound insights in the best papers covering fine tuning in LLMs?\n",
            "\u001b[36;1m\u001b[1;3m[{'id': '2403.09162v1', 'title': 'Unveiling the Generalization Power of Fine-Tuned Large Language Models', 'authors': ['Haoran Yang', 'Yumeng Zhang', 'Jiaqi Xu', 'Hongyuan Lu', 'Pheng Ann Heng', 'Wai Lam'], 'url': 'http://arxiv.org/pdf/2403.09162v1'}, {'id': '2309.13192v2', 'title': 'Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation', 'authors': ['Kai Huang', 'Hanyun Yin', 'Heng Huang', 'Wei Gao'], 'url': 'http://arxiv.org/pdf/2309.13192v2'}, {'id': '2310.10047v1', 'title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'authors': ['Yixin Liu', 'Avi Singh', 'C. Daniel Freeman', 'John D. Co-Reyes', 'Peter J. Liu'], 'url': 'http://arxiv.org/pdf/2310.10047v1'}, {'id': '2408.13296v3', 'title': 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities', 'authors': ['Venkatesh Balavadhani Parthasarathy', 'Ahtsham Zafar', 'Aafaq Khan', 'Arsalan Shahid'], 'url': 'http://arxiv.org/pdf/2408.13296v3'}, {'id': '2309.08859v1', 'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models', 'authors': ['Hongpeng Jin', 'Wenqi Wei', 'Xuyu Wang', 'Wenbin Zhang', 'Yanzhao Wu'], 'url': 'http://arxiv.org/pdf/2309.08859v1'}]\u001b[0m\n",
            "🔄 Agent Decision: Decided to use tool: search_arxiv with input: {'query': 'fine tuning in LLMs profound insights fine tuning best papers on fine tuning large language models'}\n",
            "\u001b[33;1m\u001b[1;3mThe Ultimate Guide to Fine-Tuning LLMs from\n",
            "Basics to Breakthroughs: An Exhaustive Review of\n",
            "Technologies, Research, Best Practices, Applied\n",
            "Research Challenges and Opportunities\n",
            "(Version 1.1)\n",
            "Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and\n",
            "Arsalan Shahid\n",
            "@ CeADAR Connect Group\n",
            "CeADAR: Ireland’s Centre for AI, University College Dublin, Belfield, Dublin, Ireland\n",
            "{venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid }@ ucd.ie\n",
            "October 2024arXiv:2408.13296v3  [cs.LG]  30 Oct 2024Abstract\n",
            "This technical report thoroughly examines the process of fine-tuning Large Language Models (LLMs),\n",
            "integrating theoretical insights and practical applications. It begins by tracing the historical develop-\n",
            "ment of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models\n",
            "and their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning\n",
            "methodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their\n",
            "respective implications for specific tasks.\n",
            "A structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle\n",
            "from data preparation to model deployment. Key considerations include data collection strategies,\n",
            "handling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular\n",
            "focus on hyperparameter tuning. The report also highlights parameter-efficient fine-tuning methods\n",
            "such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with\n",
            "optimal model performance.\n",
            "The exploration extends to advanced fine-tuning techniques and configurations like memory fine-\n",
            "tuning, Mixture of Experts (MoE) and Mixture of Agents (MoA), demonstrating how these methods\n",
            "harness specialised networks and multi-agent collaboration for improved outcomes. Proximal Policy\n",
            "Optimisation (PPO) and Direct Preference Optimisation (DPO) are discussed as innovative approaches\n",
            "to aligning models with human preferences, while the benefits of pruning and routing optimisations are\n",
            "examined for enhancing efficiency.\n",
            "In the latter sections, the report delves into validation frameworks, post-deployment monitoring, and\n",
            "optimisation techniques for inference. It also addresses the deployment of LLMs on distributed and\n",
            "cloud-based platforms. Additionally, cutting-edge topics such as multimodal LLMs and fine-tuning for\n",
            "audio and speech processing are covered, alongside emerging challenges related to scalability, privacy,\n",
            "and accountability.\n",
            "This report aims to serve as a comprehensive guide for researchers and practitioners, offering action-\n",
            "able insights into fine-tuning LLMs while navigating the challenges and opportunities inherent in this\n",
            "rapidly evolving field.Contents\n",
            "1 Introduction 6\n",
            "1.1 Background of Large Language Models (LLMs) . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "1.2 Historical Development and Key Milestones . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs . . . . . . . . . . . . . 6\n",
            "1.3.1 Statistical Language Models (SLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "1.3.2 Neural Language Models (NLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "1.3.3 Pre-trained Language Models (PLMs) . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "1.3.4 Large Language Models (LLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "1.4 Overview of Current Leading LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
            "1.5 What is Fine-Tuning? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
            "1.6 Types of LLM Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
            "1.6.1 Unsupervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
            "1.6.2 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
            "1.6.3 Instruction Fine-Tuning via Prompt Engineering . . . . . . . . . . . . . . . . . . . 10\n",
            "1.7 Pre-training vs Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
            "1.8 Importance of Fine-Tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
            "1.9 Retrieval Augmented Generation (RAG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
            "1.9.1 Traditional RAG Pipeline and Steps . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
            "1.9.2 Benefits of Using RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
            "1.9.3 Challenges and Considerations in Serving RAG . . . . . . . . . . . . . . . . . . . . 12\n",
            "1.9.4 Use Cases and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
            "1.9.5 Considerations for Choosing Between RAG and Fine-Tuning . . . . . . . . . . . . 12\n",
            "1.10 Objectives of the Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
            "1.10.1 Goals and Scope . . . . . . . . . . . .\u001b[0m\n",
            "🔄 Agent Decision: Decided to use tool: download_paper with input: {'paper_id': '2408.13296v3'}\n",
            "\n",
            "✅ Agent Finish: Decided to finish with output: I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and ...\n",
            "\n",
            "✨ AGENT EXECUTION COMPLETE\n",
            "\n",
            "----- DECISION LOG -----\n",
            "1. Decided to use tool: search_arxiv with input: {'query': 'fine tuning in LLMs profound insights fine tuning best papers on fine tuning large language models'}\n",
            "2. Decided to use tool: download_paper with input: {'paper_id': '2408.13296v3'}\n",
            "3. Decided to finish with output: I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and ...\n",
            "--------------------------\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "----- FINAL ANSWER -----\n",
            "I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and several profound insights have emerged across these papers. Here are some of the key takeaways:\n",
            "\n",
            "1. Deep Understanding of the Fine-Tuning Pipeline:\n",
            "   • Many of the best papers emphasize that fine-tuning is not just “tweaking” a model’s weights but involves a comprehensive, multi-stage pipeline—from careful data preparation and handling imbalanced datasets to model initialization, hyperparameter tuning, and validation. For example, in “The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs,” a seven-stage pipeline is introduced that underscores the importance of each step in ensuring robust performance.\n",
            "\n",
            "2. Parameter-Efficient Fine-Tuning:\n",
            "   • With the growing size of LLMs, the cost and computational burden of fine-tuning a full model becomes prohibitively high. As a result, techniques like Low-Rank Adaptation (LoRA) and Half Fine-Tuning have gained traction. These approaches allow for effective adaptation to new tasks while altering far fewer parameters, thus saving on both compute costs and time, and even reducing memory footprint without sacrificing much in performance.\n",
            "\n",
            "3. Advanced Techniques for Task Specialization:\n",
            "   • Beyond the standard supervised or instruction-based fine-tuning approaches, researchers are exploring advanced configurations. For example, integrating memory fine-tuning or methods inspired by mixtures-of-experts (MoE, MoA) can enable models to manage specialized sub-tasks with dedicated “mini-models” or agents within the overall architecture. This multi-faceted approach helps in focusing on particular tasks or domains, improving both accuracy and efficiency.\n",
            "\n",
            "4. Alignment with Human Preferences:\n",
            "   • Fine-tuning is increasingly seen as a way to align model outputs with human values or preferences. This is evident in approaches like Reinforcement Learning from Human Feedback (RLHF) as well as methods such as Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO). These strategies help adjust the model’s behavior post-pretraining to make its responses more useful, safe, or contextually appropriate for particular applications.\n",
            "\n",
            "5. Sustainability and Efficiency Considerations:\n",
            "   • As energy consumption and environmental impact become more critical, there’s a growing focus on “Green AI.” One recent work, “Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,” explores adaptive update schemes that reduce redundant computations and energy usage during fine-tuning. This trend underscores the need for balancing performance gains with sustainability and cost-effectiveness.\n",
            "\n",
            "6. Benchmarking and Generalization Power:\n",
            "   • Several studies, such as “Unveiling the Generalization Power of Fine-Tuned Large Language Models,” highlight that properly fine-tuned LLMs can often generalize well on downstream tasks—even beyond the suite they were specifically tuned for. This insight shows that the careful design of fine-tuning strategies not only boosts task-specific performance but also preserves or even improves the model's overall robustness.\n",
            "\n",
            "7. Integration with Retrieval Augmented Generation (RAG):\n",
            "   • There is also an interesting discussion about how fine-tuning interacts with other techniques like Retrieval Augmented Generation (RAG). While standard fine-tuning customizes the entire network, RAG incorporates external knowledge sources dynamically. Papers highlight that in some applications, a hybrid approach may yield better performance, as it leverages both the adaptability of fine-tuning and the up-to-date information that retrieval mechanisms provide.\n",
            "\n",
            "Overall, these insights reflect a maturing field where fine-tuning is evolving from a relatively “brute force” approach into a more nuanced set of strategies that incorporate efficiency, specialized adaptation, and alignment to human values. The trend is clearly moving towards methods that are not only effective in improving task performance but that also account for computational cost, sustainability, and the broader implications of deploying such large-scale models in diverse real-world settings.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and several profound insights have emerged across these papers. Here are some of the key takeaways:\\n\\n1. Deep Understanding of the Fine-Tuning Pipeline:\\n   • Many of the best papers emphasize that fine-tuning is not just “tweaking” a model’s weights but involves a comprehensive, multi-stage pipeline—from careful data preparation and handling imbalanced datasets to model initialization, hyperparameter tuning, and validation. For example, in “The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs,” a seven-stage pipeline is introduced that underscores the importance of each step in ensuring robust performance.\\n\\n2. Parameter-Efficient Fine-Tuning:\\n   • With the growing size of LLMs, the cost and computational burden of fine-tuning a full model becomes prohibitively high. As a result, techniques like Low-Rank Adaptation (LoRA) and Half Fine-Tuning have gained traction. These approaches allow for effective adaptation to new tasks while altering far fewer parameters, thus saving on both compute costs and time, and even reducing memory footprint without sacrificing much in performance.\\n\\n3. Advanced Techniques for Task Specialization:\\n   • Beyond the standard supervised or instruction-based fine-tuning approaches, researchers are exploring advanced configurations. For example, integrating memory fine-tuning or methods inspired by mixtures-of-experts (MoE, MoA) can enable models to manage specialized sub-tasks with dedicated “mini-models” or agents within the overall architecture. This multi-faceted approach helps in focusing on particular tasks or domains, improving both accuracy and efficiency.\\n\\n4. Alignment with Human Preferences:\\n   • Fine-tuning is increasingly seen as a way to align model outputs with human values or preferences. This is evident in approaches like Reinforcement Learning from Human Feedback (RLHF) as well as methods such as Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO). These strategies help adjust the model’s behavior post-pretraining to make its responses more useful, safe, or contextually appropriate for particular applications.\\n\\n5. Sustainability and Efficiency Considerations:\\n   • As energy consumption and environmental impact become more critical, there’s a growing focus on “Green AI.” One recent work, “Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,” explores adaptive update schemes that reduce redundant computations and energy usage during fine-tuning. This trend underscores the need for balancing performance gains with sustainability and cost-effectiveness.\\n\\n6. Benchmarking and Generalization Power:\\n   • Several studies, such as “Unveiling the Generalization Power of Fine-Tuned Large Language Models,” highlight that properly fine-tuned LLMs can often generalize well on downstream tasks—even beyond the suite they were specifically tuned for. This insight shows that the careful design of fine-tuning strategies not only boosts task-specific performance but also preserves or even improves the model's overall robustness.\\n\\n7. Integration with Retrieval Augmented Generation (RAG):\\n   • There is also an interesting discussion about how fine-tuning interacts with other techniques like Retrieval Augmented Generation (RAG). While standard fine-tuning customizes the entire network, RAG incorporates external knowledge sources dynamically. Papers highlight that in some applications, a hybrid approach may yield better performance, as it leverages both the adaptability of fine-tuning and the up-to-date information that retrieval mechanisms provide.\\n\\nOverall, these insights reflect a maturing field where fine-tuning is evolving from a relatively “brute force” approach into a more nuanced set of strategies that incorporate efficiency, specialized adaptation, and alignment to human values. The trend is clearly moving towards methods that are not only effective in improving task performance but that also account for computational cost, sustainability, and the broader implications of deploying such large-scale models in diverse real-world settings.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}